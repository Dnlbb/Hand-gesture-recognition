{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f434bc1b-82bf-4e2f-a13f-642687563f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': [[0.34835416, 0.86326408, 0.25937015, 0.12973758],\n",
       "  [0.5818777, 0.44759481, 0.16477829, 0.16282391]],\n",
       " 'labels': ['no_gesture', 'like'],\n",
       " 'landmarks': [[[0.35544432328385095, 0.9178467912811518],\n",
       "   [0.3895571133807179, 0.8888129819044327],\n",
       "   [0.42687296926606566, 0.8765239573841337],\n",
       "   [0.46768826637141475, 0.881365368753095],\n",
       "   [0.49627313599836925, 0.8929987321938916],\n",
       "   [0.45034106384097305, 0.9353003075967865],\n",
       "   [0.5149161960109939, 0.938208266694159],\n",
       "   [0.5508345467304484, 0.9381142225764555],\n",
       "   [0.5785308318092804, 0.9404011979190876],\n",
       "   [0.4469011545352173, 0.9599136798577973],\n",
       "   [0.5142024952392664, 0.9645932494530195],\n",
       "   [0.5538340563106309, 0.9668814567025878],\n",
       "   [0.5833119247159521, 0.9699959142001356],\n",
       "   [0.4390902744418257, 0.9747603998248608],\n",
       "   [0.49704085089653727, 0.9841366488338632],\n",
       "   [0.5334690050798111, 0.9890552768139765],\n",
       "   [0.560818492178926, 0.992685381468313],\n",
       "   [0.4304805992564116, 0.9821572823641742],\n",
       "   [0.4742554972876004, 0.9946101676168022],\n",
       "   [0.4995061984878265, 0.9994915731866427],\n",
       "   [0.5196331621910297, 1.003428687870273]],\n",
       "  [[0.7445607738703431, 0.5649720888661142],\n",
       "   [0.7251958432153897, 0.5285693270127082],\n",
       "   [0.6918738657843627, 0.5015717846113269],\n",
       "   [0.6636059536864003, 0.4790815689221386],\n",
       "   [0.6523250653989778, 0.4583007922695633],\n",
       "   [0.6398442919441839, 0.517223525181593],\n",
       "   [0.5960129215271969, 0.5279911722380395],\n",
       "   [0.6141771765655129, 0.5332147721205427],\n",
       "   [0.6333531562098078, 0.5304248562098982],\n",
       "   [0.6383877630364788, 0.5413212245656618],\n",
       "   [0.5992484994611111, 0.5494568647703711],\n",
       "   [0.6182417675985264, 0.5527000108355533],\n",
       "   [0.6367112369618471, 0.5497970018166604],\n",
       "   [0.6434610514229644, 0.5651706261763787],\n",
       "   [0.608356308172372, 0.5698216969284559],\n",
       "   [0.6254726797553172, 0.5713704897653529],\n",
       "   [0.6442495329746062, 0.5688618308397323],\n",
       "   [0.6528448278363963, 0.5868224947379036],\n",
       "   [0.6238116263046112, 0.5884535279345797],\n",
       "   [0.6388656148447419, 0.5878808474415285],\n",
       "   [0.655075156121083, 0.5854825823240042]]],\n",
       " 'leading_conf': 1.0,\n",
       " 'leading_hand': 'left',\n",
       " 'user_id': '4d5563fab555d17ab3a4347a95e56928'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/home/daniil/Документы/like.json'\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    like_dataset = json.load(file)\n",
    "\n",
    "sample_key = next(iter(like_dataset.keys()))\n",
    "like_dataset[sample_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce1c630-f115-437e-9bdd-4445e622dde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 21}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "landmark_lengths = []\n",
    "\n",
    "for key, value in like_dataset.items():\n",
    "    if value['labels'] and value['landmarks']:\n",
    "        for landmarks in value['landmarks']:\n",
    "            landmark_lengths.append(len(landmarks))\n",
    "\n",
    "\n",
    "unique_lengths = set(landmark_lengths)\n",
    "unique_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df51905-d1c2-4ded-bc64-dc56a36e72cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cleaned_features = []\n",
    "cleaned_labels = []\n",
    "\n",
    "for key, value in like_dataset.items():\n",
    "    if value['labels'] and value['landmarks']:\n",
    "        index = 0 if value['leading_hand'] == 'left' else 1\n",
    "        if index < len(value['landmarks']) and len(value['landmarks'][index]) == 21:\n",
    "            cleaned_features.append(np.array(value['landmarks'][index]).flatten())\n",
    "            cleaned_labels.append(1 if 'like' in value['labels'][index] else 0)\n",
    "\n",
    "cleaned_features = np.array(cleaned_features)\n",
    "cleaned_labels = np.array(cleaned_labels)\n",
    "\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(\n",
    "    cleaned_features, cleaned_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "\n",
    "accuracy_rf = model_rf.score(X_test_cleaned, y_test_cleaned)\n",
    "accuracy_rf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bc3c5-747b-4e16-9f94-7d5b371b8f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 16:33:20.085810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 16:33:20.833334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713447202.136738    5030 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1713447202.140020    5135 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: REMBRANDT (rembrandt, LLVM 15.0.7, DRM 3.54, 6.5.0-27-generic)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Игнорирование пустого кадра камеры.\")\n",
    "            continue\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "       \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                landmarks = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "                gesture_prediction = model_rf.predict(landmarks)\n",
    "                gesture_label = '+' if gesture_prediction == 1 else '-'\n",
    "\n",
    "               \n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS)\n",
    "                cv2.putText(image, gesture_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72070fc6-8723-4b6c-b799-cf80f6df20df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6831b0-0e03-4b97-9355-1a5bd1f73eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
